{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# References\n",
      "\n",
      "1. Probability Theory / Bayesian Methods\n",
      "\n",
      "    1. Introduction to Probability and Its Applications (2nd ed.), Richard L. Scheaffer [Amazon](http://www.amazon.com/Introduction-Probability-Its-Applications-Statistics/dp/0534237908/ref=sr_1_2?ie=UTF8&qid=1426086883&sr=8-2&keywords=Richard+Scheaffer+%2Bintroduction)\n",
      "\n",
      "1. Probability Theory / Matched Filtering\n",
      "\n",
      "    2. Gravitational-Wave Physics and Astronomy, Anderson and Creighton [Amazon](http://www.amazon.com/Gravitational-Wave-Physics-Astronomy-Introduction-Experiment/dp/352740886X/ref=sr_1_1?ie=UTF8&qid=1426087323&sr=8-1&keywords=gravitational+wave+physics+and+astronomy)\n",
      "\n",
      "1. Matched Filtering / Gravitational-Wave Data Analysis\n",
      "\n",
      "    3. Gravitational-Wave Physics and Astronomy, Anderson and Creighton [Amazon](http://www.amazon.com/Gravitational-Wave-Physics-Astronomy-Introduction-Experiment/dp/352740886X/ref=sr_1_1?ie=UTF8&qid=1426087323&sr=8-1&keywords=gravitational+wave+physics+and+astronomy)\n",
      "    4. Gravitational Waves (vol. 1), Michele Maggiore [Amazon](http://www.amazon.com/Gravitational-Waves-1-Theory-Experiments/dp/0198570740/ref=sr_1_1?ie=UTF8&qid=1426089182&sr=8-1&keywords=gravitational+waves+maggiore)\n",
      "    5. Search for Gravitational-Wave Radiation from Binary Black Hole MACHOs in the Galactic Halo (Ph.D. thesis), Duncan A. Brown [arXiv](http://arxiv.org/pdf/0705.1514v1.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Statement of the Problem\n",
      "\n",
      "We seek to answer the following question:\n",
      "\n",
      "*How do I optimally search for a signal in noise, given that the signal and the noise can be modeled with a finite number of model parameters*\n",
      "\n",
      "From here on out, I'll describe model parameters with $\\vec{\\mu}$. Note that this set of parameters is *model specific*, that is if I change my model (e,g. change how the signal is generated from the model), the parameters will not give the same realization of the signal. You find that difference academic, but recall we do not have a complete and accurate description of a gravitational radiation waveform from a binary inspiral, so even little differences in how we approximate certain processes can amount to a completely different description of the signal. We'll see later how we can use that to our advantage in terms of ''model selection''.\n",
      "\n",
      "The one added complication here is that the noise, while describable with a finite set of parameters, is not a fixed number itself. We only get realizations of the underlying random process which creates the noise. We call one measurement of this distribution a _noise realization_. Therefore, we've launched ourselves out of the realm of deterministic equations and into probability theory. Each statement we make will be probabilitstic because we do not know a priori what noise realization we will obtain: we can only give a distribution of values that we are likely to obtain.\n",
      "\n",
      "## Bayes' Law\n",
      "\n",
      "Questions that follow the form, 'How do I obtain A given a set of data described by B' are the bread and butter of Bayesian statistics. So, let's start off axiomatically. Here's Bayes's Law:\n",
      "\n",
      "$$P(A|B) = \\frac{P(B|A)P(A)}{\\sum_i P(B|A_i)P(B)}$$\n",
      "\n",
      "I've written the denominator out so as to refer to it later. The sum is replaced by an integral when the $A_j$ goes from discrete to continuous. A slightly more convenient form is given as:\n",
      "\n",
      "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
      "\n",
      "It says, in words, the probability of $A$ given that $B$ is true is equal to the probability of $B$ being true given $A$ weighted by the probability of $A$ and normalizated by the probability of $B$. You can do a quick (though somewhat circular) proof of this just by taking the ratio of two conditional probabilities $P(A) = P(B and A)/P(B)$. This image might be a helpful guide:\n",
      "\n",
      "<a title=\"By Gnathan87 (Own work) [CC0], via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File%3AProbability_tree_diagram.svg\"><img width=\"256\" alt=\"Probability tree diagram\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Probability_tree_diagram.svg/256px-Probability_tree_diagram.svg.png\"/></a>\n",
      "\n",
      "Note the $\\cap$ symbol means \"intersection\" and is equivalent to the \"and\" operator in set or logic theory.\n",
      "\n",
      "## Hypothesis Testing\n",
      "\n",
      "This is all well and good, but how does one actually use this. Well, as we had just posited, we have a quesiton that we want to determine A given B. Let's assign meaning to this. I want to know the probability of obtaining a signal ($h$) with parameters $\\vec{\\mu}$ with a given data which contains a possible *true* signal ($h_0$) and a parameterized but otherwise unspecified noise model $n$. Strictly speaking, we can assign two hypothesis:\n",
      "\n",
      "$H_0$: The _null_ hypothesis. The data contains only noise, not a signal. This statement is, physically speaking, equivalent to having a signal present with an amplitude of 0, but the distinction between this and the signal $A=0$ hypothesis is subtle and I'm going to overlook it.\n",
      "\n",
      "$H_1$: The data contains noise (parameterized is the *same* way as $H_0$) *and* a non-zero amplitude signal $h_0$ (with parameters $\\vec{\\mu_0}$ which I wish to detect.\n",
      "\n",
      "In plainer terms, we're searching for a signal in randomized noise and we want to quantify things about that signal using probability distributions. What we measure, and can use to do inference about the parameters is a data set $D$: it's our \"truth\".Ultimately, we're looking for the _posterior_: the probability $p(\\vec{\\mu},H_1|D)$. On the way, we're going to learn about matched filtering because it becomes a natural component of this analysis. Let's insert these realities into the problem:\n",
      "\n",
      "$$p(\\vec{\\mu},H_1|D) = \\frac{p(\\vec{\\mu},H_1)p(D|\\vec{\\mu},H_1)}{p(D)}$$\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "$p(D)$: This is the probability of obtaining a given data set $D$. Let's simplify the analysis here. We just said we must be able to  parameterize the noise model. While there are many ways to do this, the most simplifying assumption we can make is that the noise are independent samples drawn from a Gaussian distribution with the same mean $\\mu$ (not to be confused with our signal parameters) and variance $\\sigma^2$. Therefore getting a certain realization of noise is a time series of independent Gaussian variables, and the probability of obtaining the series in general is:\n",
      "\n",
      "$$p(D) \\propto \\prod_i^N \\exp(-(x_i-\\mu)^2/2/\\sigma^2)$$\n",
      "\n",
      "As an aside: we can efficiently represent this as an inner product. You might recall this notation from quantum mechanics: (X|Y). For properly normalized quantities, (X|X) = 1. You might also recall the expectation value of an operator, defined as (X|O|Y). In all further discussion, when I write (X|Y), it's implied that the inner product is done in the frequency domain, and the implied operator is a weighting function, in this case the power spectral density. There's all kinds of subtleties and derivations I'm not even going into here. Just know that Gaussian random variables in the time domain are also Gaussian random variables in the frequency domain. Just remember that (D|D) is still the product of the Gaussian probabilities. So, I'm going to write this a little more compactly:\n",
      "\n",
      "$$p(D) \\propto \\exp(-(D|D)/2)$$\n",
      "\n",
      "Further note that this is also the posterior probability for $p(H_0|D)$.\n",
      "\n",
      "$p(\\vec{\\mu},H_1)$: Read aloud, this would be, \"the probability of obtaining a parameter set realization $\\mu$ under the $H_1$ hypothesis. This needs careful thought. Several competing models (or hypothesis) $H_n$ can (and often will) have the same parameter labels. Some may include more, some less. In this case we are _quantifying our belief in the hypothesis_ $H_1$ by assigning it a probability to occur under this hypothesis. This quantity is called the _prior_: it quantifies, in some way, the underlying distirbution of the parameters themselves. It's affected by our present knowledge and belief in the physics and astronomy.\n",
      "\n",
      "### Brief digression: priors\n",
      "\n",
      "Let's take a digression and examine something concrete.\n",
      "\n",
      "In the case where we have no pre-existing knowledge or belief in a certain parameter, we can assign it an ['uninformative' prior](https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors): usually a uniform distribution (though you can construct [others](https://en.wikipedia.org/wiki/Jeffreys_prior). There is some danger in the details here since the uniform distribution still affects the shape of the posterior by imposing limits on range of values. There's a lot to consider here, but in our case we're going to gloss over these.\n",
      "\n",
      "Let's do a brief example of a more physical case. Given the current state of thought on cosmology and the distribution of matter in the universe, we are relatively ocnfident that in broad terms, the distribution of compact objects in the universe is isotropic. This is true up to effects from cosmology (e.g. an expanding universe). We do not expect to be able to see this effect with advanced LIGO, so it's safe to say our sources are isotropic in the *nearby* universe. So if one of the parmaeters we wish to maesure is the _distance_ to the binary, then we must assign it a prior. We just stated that the sources are isotropically distributed in the nearby universe, so the _distribution_ of distance parameters (encoded in the waveform signal as $1/d$ and usually related intrinsically to the amplitude of the signal when detected) if we could measure all of them exactly would be proportional to $d^2$. E.g.:\n",
      "\n",
      "$$p(d) \\propto d^2$$\n",
      "\n",
      "Integrating this prior to normalize it means imposing a cutoff distance, since the integral would not converge ar $d=\\infty$. We again appeal to common sense, because we know that our instruments cannot possibly detect signals beyond a certain range (if we believe nominal models of signal generation), so as long as we put our fiducial cutoff beyond this radius, we have not significantly biased ourselves.\n",
      "\n",
      "As you can see, selection of the prior has both subtle and gross effects on the shape of your posterior, and it's important to understand the basic physics and major assumptions which make up your model and hypothesis. Of course, if you question and of the underpinned assumptions here, you could create a *new* hypothesis to test *that* assumption. Say it with me: \"All hail Rev. Bayes!\"\n",
      "\n",
      "<a title=\"See page for author [Public domain], via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File%3AThomas_Bayes.gif\"><img width=\"256\" alt=\"Thomas Bayes\" src=\"//upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif\"/></a>\"S'up?\"\n",
      "\n",
      "## Matched Filtering\n",
      "\n",
      "Before continuing, I've neglected a full description of $D$ under our two hypotheses.\n",
      "\n",
      "$$H_0\\rightarrow D(f) = n(f)$$\n",
      "$$H_1\\rightarrow D(f) = n(f)+h(f;\\vec{\\mu})$$\n",
      "\n",
      "That is to say, $D$ in the frequency domain is purely Gaussian noise under $H_0$ and Gaussian noise plus a signal $h(f)$ under $H_1$. We neglected to describe one term in the above Bayesian' formula, and we're in a position to do that effectively now, so let's do it:\n",
      "\n",
      "$p(D|\\vec{\\mu},H_1)$: The probability that the data contains a signal with hypothesis $H_1$. This is:\n",
      "\n",
      "$$p(D) \\propto \\exp(-(D-h(\\vec{\\mu})|D-h(\\vec{\\mu})/2)$$\n",
      "\n",
      "That is to say: when I subtract off my putative signal from the data, do I obtain my noise model back? More specifically, are the residuals consistent with the $H_0$ hypothesis? This motivates us to do some trickery (really... like I need a motivation to sweep things under the rug). If I divide both sides of the ratio by p(H_0|D), I have:\n",
      "\n",
      "$$p(\\vec{\\mu},H_1|D) = \\frac{p(\\vec{\\mu},H_1)p(D|\\vec{\\mu},H_1)/p(D|H_0)}{p(D)/p(D|H_0)}$$\n",
      "\n",
      "Recall from first digression (the one about $A=0$) that we didn't make a distinction between a parameter set which would give us only noise back, and the hypothesis of only noise. That gives us leave (though not a lot) to write the overall denominator of the ratio as\n",
      "\n",
      "$$Z=\\frac{p(D|H_1)}{p(D|H_0)}$$\n",
      "\n",
      "and I've played a little fast and loose here with the $H_1$ and $H_0$ labels. You might be able to convince yourself that none of this matters too much since the quantities that involve H_1 invariably also have $\\vec{\\mu}$ dependence, so the quantities that involve $H_0$ only concern themselves with the *noise* which we are not so interested in. In effect, we're dividing out the contribution to the ratio of the noise as much as we can. \n",
      "\n",
      "Not accounting for the prior which remains unchanged, we can expand out the meaning of the ratio in the numerator:\n",
      "\n",
      "$$\\Lambda(\\vec{\\mu})=\\frac{p(D|\\vec{\\mu},H_1)}{p(D|H_0)}=\\exp(-(h(\\vec{\\mu})|D)-(h(\\vec{\\mu})|h(\\vec{\\mu})/2)$$\n",
      "\n",
      "This is called the *likelihood ratio*. It is, in some manner of speaking, the fundamental quantity we manipulate and calculate when doing data analysis. It is a direct comparison of probability that a signal is present versus the probability it is not. In the case where $h$ is a given, e.g. it is fixed, the measurement to be made is the data with the known model $(h|D)$. Inner products have a maximum when the two entities are identical. Recall $(h|h)$ is simply a constant, in effect, this says that when one wants to search for a given signal $h$ in data $D$, the optimal way to do so is to take the inner product of the data with the signal. This is called _matched filtering_.\n",
      "\n",
      "The quantity $(h|h)$ is called the ''characteristic'' or ''matched filter'' signal to noise ratio. It is integrated, noise-weighted power collected by an instrument from a signal $h$ in the limit of zero noise. That last piece of the statement isn't wholly sensical, but you can think of it as the number we'd measure if the instrument noise were turned way down. The quantity $(h|D)$ is the *realized* signal to noise ratio (SNR), in other words what we *actually* measure. You can see how one becomes the other as the noise becomes very small. The realized SNR is a fundamentally random quantity, though since the noise is included in the measurement. We expect that if we could do several realizations of the noise that the measurement of $(h|D)$ would be peaked near $(h|h)$, and furthermore, the width of that distribution arises *only* from the noise, which we've described as a Gaussian, so it does not change in resposne to the signal (at least we hope it doesn't, since that would imply the noise and signal are correlated, and that's bad news bears).\n",
      "\n",
      "<img src=\"https://upload.wikimedia.org/wikipedia/en/c/c9/Bad_News_Bears_film.jpg\"> This is apparently an actual thing.\n",
      "\n",
      "Anyway. So that's it for matched filtering. Pretty much all of our searches use this in some way or another to define a quantity of interest when doing the search. The trick becomes how well you can specify your model (e.g. how many signal templates do I need to cover all possiblities that nature can throw at me) and how kind nature is in providing said signal. This suggests a _bank_ of signal templares to cover all possible $\\vec{\\mu}$ efficiently, but that's a topic for another time.\n",
      "\n",
      "## Posterior Measurement and Bayesian Inference\n",
      "\n",
      "So we're now down to this quantity:\n",
      "\n",
      "$$p(\\vec{\\mu}|D) = \\frac{p(\\vec{\\mu})\\Lambda(\\vec{\\mu}|D)}{Z}$$\n",
      "\n",
      "You'll note I've stopped writing things like $H_1$. It's now implied that we've fixed our model and the only things we're measuring for the time being are $\\vec{\\mu}$ or, equivalently, the $h$ waveform they would produce. I'll return to the denominator in a moment, however, let's take a brief moment to inspect the numerator. It consists of *things we know* or at least know how to calculate from the data: the priors (which we specify in advance from our physical knowledge) and the likelihood ratio which we measure from the data given a specific signal model. So we've got that down. What about the denominator? Well, in the official parlance this is called the _odds ratio_ (remember it's actually the ratio of the two conditional model hypothesis probabilities. Let's examine it closely:\n",
      "\n",
      "$$Z=\\frac{p(D|H_1)}{p(D|H_0)}=\\frac{\\int p(D|\\vec{\\mu},H_1)p(\\vec{\\mu})d\\vec{\\mu}}{p(D|H_0)}$$\n",
      "\n",
      "Refer back to the first form of Bayes' Law that I stated initially, I've replaced that summation/integral back in explicitly. We could have carried it around, but it was a bit cumbersone, and it's really only useful now. Now, again remamber that $p(D|H_0)$ has no dependence on $\\vec{\\mu}$, so it's a constant in relation to the integral, and now one can see that you actually have this:\n",
      "\n",
      "$$Z=\\int p(\\vec{\\mu})\\Lambda(\\vec{\\mu})d\\vec{\\mu}$$\n",
      "\n",
      "...and its appearance as an overall normalization factor is now crystal clear. So, to get our posterior, we evaluate the likelihood ratio weighted by the prior (our ''measurement'' weighted by our ''belief'') and then normalize the quantity appropriately!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lal\n",
      "import lalsimulation\n",
      "\n",
      "delta_t = 1.0/4096\n",
      "hp, hx = lalsimulation.SimInspiralChooseTDWaveform(0, delta_t, # reference phase, delta_t\n",
      "                                          10 * lal.MSUN_SI, 10 * lal.MSUN_SI, # m1, m2\n",
      "                                          0, 0, 0, # s1vec\n",
      "                                          0, 0, 0, # s2vec\n",
      "                                          40, 0, # minimum frequency, reference frequency\n",
      "                                          1 * lal.PC_SI * 1e6, # distance (m)\n",
      "                                          numpy.pi/2, # inclination (face on)\n",
      "                                          0, 0, # tidal terms\n",
      "                                          None, # waveform flags\n",
      "                                          None, # non-GR flags\n",
      "                                          -1, 7, lalsimulation.GetApproximantFromString(\"TaylorT4\") # amplitude order, phase order, approximant\n",
      "                                          )\n",
      "print len(hp.data.data)\n",
      "#t = numpy.range(len(hp.data.data))*delta_t\n",
      "#pyplot.plot(t, hp, 'b-')\n",
      "#pyplot.plot(t, hx, 'r-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}